{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vision Transformer (ViT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vision transformer applies a pure transformer to images without any convolution layers. They split the image into patches and apply a transformer on patch embeddings. Patch embeddings are generated by applying a simple linear transformation to the flattened pixel values of the patch. Then a standard transformer encoder is fed with the patch embeddings, along with a classification token [CLS] . The encoding on the [CLS] token is used to classify the image with an MLP.\n",
    "\n",
    "When feeding the transformer with the patches, learned positional embeddings are added to the patch embeddings, because the patch embeddings do not have any information about where that patch is from. The positional embeddings are a set of vectors for each patch location that get trained with gradient descent along with other parameters.\n",
    "\n",
    "ViTs perform well when they are pre-trained on large datasets. The paper suggests pre-training them with an MLP classification head and then using a single linear layer when fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "from labml_helpers.module import Module\n",
    "from labml_nn.transformers import TransformerLayer\n",
    "from labml_nn.utils import clone_module_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get patch embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The paper splits the image into patches of equal size and do a linear transformation on the flattened pixels for each patch.\n",
    "\n",
    "We implement the same thing through a convolution layer, because it's simpler to implement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchEmbeddings(Module):\n",
    "    \"\"\"\n",
    "    <a id=\"PatchEmbeddings\"></a>\n",
    "\n",
    "    ## Get patch embeddings\n",
    "\n",
    "    The paper splits the image into patches of equal size and do a linear transformation\n",
    "    on the flattened pixels for each patch.\n",
    "\n",
    "    We implement the same thing through a convolution layer, because it's simpler to implement.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, d_model: int, patch_size: int, in_channels: int):\n",
    "        \"\"\"\n",
    "        * `d_model` is the transformer embeddings size\n",
    "        * `patch_size` is the size of the patch\n",
    "        * `in_channels` is the number of channels in the input image (3 for rgb)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        # We create a convolution layer with a kernel size and and stride length equal to patch size.\n",
    "        # This is equivalent to splitting the image into patches and doing a linear\n",
    "        # transformation on each patch.\n",
    "        self.conv = nn.Conv2d(in_channels, d_model, patch_size, stride=patch_size)\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        \"\"\"\n",
    "        * `x` is the input image of shape `[batch_size, channels, height, width]`\n",
    "        \"\"\"\n",
    "        # Apply convolution layer\n",
    "        x = self.conv(x)\n",
    "        # Get the shape.\n",
    "        bs, c, h, w = x.shape\n",
    "        # Rearrange to shape `[patches, batch_size, d_model]`\n",
    "        x = x.permute(2, 3, 0, 1)\n",
    "        x = x.view(h * w, bs, c)\n",
    "\n",
    "        # Return the patch embeddings\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add parameterized positional encodings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This adds learned positional embeddings to patch embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LearnedPositionalEmbeddings(Module):\n",
    "    \"\"\"\n",
    "    <a id=\"LearnedPositionalEmbeddings\"></a>\n",
    "\n",
    "    ## Add parameterized positional encodings\n",
    "\n",
    "    This adds learned positional embeddings to patch embeddings.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, d_model: int, max_len: int = 5_000):\n",
    "        \"\"\"\n",
    "        * `d_model` is the transformer embeddings size\n",
    "        * `max_len` is the maximum number of patches\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        # Positional embeddings for each location\n",
    "        self.positional_encodings = nn.Parameter(torch.zeros(max_len, 1, d_model), requires_grad=True)\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        \"\"\"\n",
    "        * `x` is the patch embeddings of shape `[patches, batch_size, d_model]`\n",
    "        \"\"\"\n",
    "        # Get the positional embeddings for the given patches\n",
    "        pe = self.positional_encodings[:x.shape[0]]\n",
    "        # Add to patch embeddings and return\n",
    "        return x + pe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP Classification Head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the two layer MLP head to classify the image based on [CLS] token embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassificationHead(Module):\n",
    "    \"\"\"\n",
    "    <a id=\"ClassificationHead\"></a>\n",
    "\n",
    "    ## MLP Classification Head\n",
    "\n",
    "    This is the two layer MLP head to classify the image based on `[CLS]` token embedding.\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model: int, n_hidden: int, n_classes: int):\n",
    "        \"\"\"\n",
    "        * `d_model` is the transformer embedding size\n",
    "        * `n_hidden` is the size of the hidden layer\n",
    "        * `n_classes` is the number of classes in the classification task\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        # First layer\n",
    "        self.linear1 = nn.Linear(d_model, n_hidden)\n",
    "        # Activation\n",
    "        self.act = nn.ReLU()\n",
    "        # Second layer\n",
    "        self.linear2 = nn.Linear(n_hidden, n_classes)\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        \"\"\"\n",
    "        * `x` is the transformer encoding for `[CLS]` token\n",
    "        \"\"\"\n",
    "        # First layer and activation\n",
    "        x = self.act(self.linear1(x))\n",
    "        # Second layer\n",
    "        x = self.linear2(x)\n",
    "\n",
    "        #\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vision Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This combines the patch embeddings, positional embeddings, transformer and the classification head."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VisionTransformer(Module):\n",
    "    \"\"\"\n",
    "    ## Vision Transformer\n",
    "\n",
    "    This combines the [patch embeddings](#PatchEmbeddings),\n",
    "    [positional embeddings](#LearnedPositionalEmbeddings),\n",
    "    transformer and the [classification head](#ClassificationHead).\n",
    "    \"\"\"\n",
    "    def __init__(self, transformer_layer: TransformerLayer, n_layers: int,\n",
    "                 patch_emb: PatchEmbeddings, pos_emb: LearnedPositionalEmbeddings,\n",
    "                 classification: ClassificationHead):\n",
    "        \"\"\"\n",
    "        * `transformer_layer` is a copy of a single [transformer layer](../models.html#TransformerLayer).\n",
    "         We make copies of it to make the transformer with `n_layers`.\n",
    "        * `n_layers` is the number of [transformer layers](../models.html#TransformerLayer).\n",
    "        * `patch_emb` is the [patch embeddings layer](#PatchEmbeddings).\n",
    "        * `pos_emb` is the [positional embeddings layer](#LearnedPositionalEmbeddings).\n",
    "        * `classification` is the [classification head](#ClassificationHead).\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        # Patch embeddings\n",
    "        self.patch_emb = patch_emb\n",
    "        self.pos_emb = pos_emb\n",
    "        # Classification head\n",
    "        self.classification = classification\n",
    "        # Make copies of the transformer layer\n",
    "        self.transformer_layers = clone_module_list(transformer_layer, n_layers)\n",
    "\n",
    "        # `[CLS]` token embedding\n",
    "        self.cls_token_emb = nn.Parameter(torch.randn(1, 1, transformer_layer.size), requires_grad=True)\n",
    "        # Final normalization layer\n",
    "        self.ln = nn.LayerNorm([transformer_layer.size])\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        \"\"\"\n",
    "        * `x` is the input image of shape `[batch_size, channels, height, width]`\n",
    "        \"\"\"\n",
    "        # Get patch embeddings. This gives a tensor of shape `[patches, batch_size, d_model]`\n",
    "        x = self.patch_emb(x)\n",
    "        # Concatenate the `[CLS]` token embeddings before feeding the transformer\n",
    "        cls_token_emb = self.cls_token_emb.expand(-1, x.shape[1], -1)\n",
    "        x = torch.cat([cls_token_emb, x])\n",
    "        # Add positional embeddings\n",
    "        x = self.pos_emb(x)\n",
    "\n",
    "        # Pass through transformer layers with no attention masking\n",
    "        for layer in self.transformer_layers:\n",
    "            x = layer(x=x, mask=None)\n",
    "\n",
    "        # Get the transformer output of the `[CLS]` token (which is the first in the sequence).\n",
    "        x = x[0]\n",
    "\n",
    "        # Layer normalization\n",
    "        x = self.ln(x)\n",
    "\n",
    "        # Classification head, to get logits\n",
    "        x = self.classification(x)\n",
    "\n",
    "        #\n",
    "        return x"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
